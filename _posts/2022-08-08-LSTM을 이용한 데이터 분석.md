# RNN을 이용한 시계열 데이터 분석

## Ⅰ. Intro

 Machine learning을 이용하여 시간축이 있는 데이터를 학습하기 위해서는 어느 데이터의 이전 시간, 그리고 다음 시간의 정보가 반영되어야만 한다. 하지만 기존의 NN은 한 샘플의 전후 데이터가 반영되지 않는 바, 이러한 시계열 데이터를 다루기 위한 새로운 model이 고안되었으니 이것이 Recurrent Neural Network(이하 RNN)이다. 이하 본문에서는 RNN 및 RNN에 속하는 유용한 model을 이용하여 시계열 데이터 분석 과정을 소개한다.

## Ⅱ. Recurrent Nerual Network

(이하 내용은 "딥 러닝을 이용한 자연어 처리 입문" https://wikidocs.net/2288 을 기반으로 스스로 이해한 바를 작성하였습니다.)

 ![1](https://github.com/44isOutlaw/44isOutlaw.github.io/blob/master/assets/img/RNN/rnn_image1_ver2.PNG?raw=true)

 RNN의 특징을 우선 설명하면, 위 그림과 같이 셀(cell)이 있다는 점이다. 셀은 이전의 값을 기억하려고 하고, 이를 다음 셀로 전달한다. 즉 이전 시간의 정보를 다음 시간의 정보 학습에 반영시킬 수 있다는 것이다.

![2](https://github.com/44isOutlaw/44isOutlaw.github.io/blob/master/assets/img/RNN/rnn_image4_ver2.PNG?raw=true)                           ![3](https://github.com/44isOutlaw/44isOutlaw.github.io/blob/master/assets/img/RNN/rnn_image2_ver3.PNG?raw=true)

 좌측 그림과 같이 현재의 셀은 이전 시간 셀로부터 h라는 값을 받으며, 이를 은닉 상태(hidden state)라고 표현한다. 전달받은 과거의 은닉 상태(h<sub>t-1</sub>)는 현재의 은닉 상태(h<sub>t</sub>)를 계산하는데 사용된다. 계산 과정은 NN와 같이 가중치(w) 및 bias(b)가 추가되어 activation function을 거치는 과정을 거친다. 이렇게 얻은 현재의 h는 출력값 y를 계산하는데 사용된다. 이와 같은 특징을 살려 시계열 데이터의 학습이 가능해지며, 데이터의 분석 뿐만 아니라 다음 시간의 출력값 예측(예를 들어, 주가 동향) 등 다양한 상황에 적용할 수 있다.

![4](https://github.com/44isOutlaw/44isOutlaw.github.io/blob/master/assets/img/RNN/lstm_image1_ver2.PNG?raw=true)

 하지만 위와 같은 방식의 RNN에는 단점이 있는데, 많은 시간 간격을 지날수록 초기의 은닉 상태값의 특징이 옅어진다는 점이다. 이는 곧 시간이 길어질수록 초반부의 데이터가 반영되지 않음을 의미하며 의도한 바와 다르게 model의 학습되는 결과를 야기한다. 이를 장기 의존성 문제(the problem of Long-Term Dependencies)라고 한다. 이를 해결하기 위해, RNN을 수정한 모델인 LSTM, GRU 등이 등장하게 된다.

## Ⅲ. LSTM, GRU

 시계열 데이터를 다루는 가장 대표적인 두 모델인 LSTM, GRU에 대하여 알아보자.

![5](https://github.com/44isOutlaw/44isOutlaw.github.io/blob/master/assets/img/RNN/vaniila_rnn_and_different_lstm_ver2.PNG?raw=true)

 위 그림은 Long Short-Term Memory(이하 LSTM)의 구조이다. LSTM은 기존 은닉 상태(h)와 새롭게 추가된 셀 상태(C)를 삭제 게이트, 입력 게이트, 출력 게이트를 통해 계산하여 불필요한 정보를 제거하고 기억해야 할 정보를 다음 상태로 전달시키는 과정을 수행한다. 현재의 셀 상태(C<sub>t</sub>)는 삭제 게이트를 거쳐온 이전 셀 상태(C<sub>t-1</sub>)와 입력 게이트를 거쳐온 이전 은닉 상태(h<sub>t-1</sub>)와 현재 입력값(x <sub>t</sub> )을 통해 계산되며, 현재의 은닉 상태(h<sub>t</sub>)는 이전 은닉 상태(h<sub>t-1</sub>)와 현재 입력값(x<sub>t</sub>)를 통해 계산되며, 최종 output에 도달할 때까지 반복된다. LSTM은 이와 같이 게이트들이 정보를 선별하면서 장기 의존성 문제를 완화하여, 초기 시간의 중요 정보들을 보존하여 학습을 수행할 수 있다.

​                                                 ![6](https://github.com/44isOutlaw/44isOutlaw.github.io/blob/master/assets/img/RNN/gru.PNG?raw=true)

  한편, Gated Recurrent Unit(이하 GRU)의 구조는 위 그림과 같다. GRU는 LSTM과 달리 업데이트 게이트와 리셋 게이트를 갖고 있다. 두 게이트 모두 이전 은닉 상태(h<sub>t-1</sub>)와 현재 입력값(x <sub>t</sub> )이 지나가게 되는데, 업데이트 게이트를 지난 값(z<sub>t</sub>)이 이전 은닉 상태(h<sub>t-1</sub>)와 리셋 게이트와 현재 입력값(x <sub>t</sub> )을 거쳐 연산된 g<sub>t</sub>의 비중을 조절하여 현재 은닉 상태(h<sub>t</sub>)를 출력한다. 이와 같은 방식으로 이전의 중요 정보들을 보전하여 장기 의존성 문제를 완화한다. GRU는 LSTM과 비슷한 성능을 갖고 있으므로, LSTM의 최적화를 마쳤다면 굳이 GRU로 바꿀 필요가 없다고 한다.

## Ⅳ. Keras의 LSTM을 이용한 시계열 데이터 분석

 이번 model의 목표는 position, velocity를 입력 받으면 energy 소모량을 예측할 수 있도록 하는 것이다. 이하에는 그 과정을 기록한다.

### 시계열 데이터의 처리

| time | position | velocity | energy |
| ---- | -------- | -------- | ------ |
| 0    | 0        | 0        | 0      |
| 1    | 15       | 100      | 1000   |
| 2    | 67       | 360      | 3000   |
| ...  | ...      | ...      | ...    |

 오늘 사용하는 데이터는 공개하기에는 어려움이 있어, 예시로만 설명을 하고자 한다. 이 데이터는 시간에 따라 어떤 기계의 위치, 속도의 변화에 따른 에너지 소모량을 기록한 것이다. 이와 같이 일정 시간 간격마다 기록된 데이터를 **시계열 데이터**라고 한다. 이번엔 이렇게 생긴 엑셀 파일(.xlsx)을 구글 드라이브에서 import 받을 것이다.

```python
from google.colab import drive
drive.mount('/content/drive')
```

  위와 같은 코드를 통해 구글 드라이브에 접근할 수 있다. 이 코드를 작성하면 보안 알림이 뜨는데 모두 예를 누르면 된다. 이렇게 입력하고 colab의 좌측 파일 모양의 아이콘을 누르면,

![data1](https://github.com/44isOutlaw/44isOutlaw.github.io/blob/master/assets/img/RNN/data1.PNG?raw=true)

이와 같이 drive가 추가된 것을 볼 수 있다. 여기서는 내부 파일에 있는 MyDrive에 xlsx 파일을 업로드하여 import 하였으며, 사용된 코드는 아래와 같다.

```python
import pandas as pd

data = [0]*3
data[0] = pd.read_excel("/content/drive/MyDrive/data.xlsx", engine = 'openpyxl', sheet_name='Sheet1')
data[1] = pd.read_excel("/content/drive/MyDrive/data.xlsx", engine = 'openpyxl', sheet_name='Sheet2')
data[2] = pd.read_excel("/content/drive/MyDrive/data.xlsx", engine = 'openpyxl', sheet_name='Sheet3')
```

 pd.read_excel을 통해 xlsx 파일을 pandas의 dataframe으로 받을 수 있다. 엑셀 파일이 여러 개의 sheet로 나뉘어 있다면 sheet_name을 인자로 하나씩 저장할 수 있다. 위 파일은 3개의 sheet가 있으며, data라는 이름의 list를 정의하여 하나씩 저장하였다.

```python
import numpy as np

x1 = data[0][['position', 'velocity']].fillna(0)
x2 = data[1][['position', 'velocity']].fillna(0)
x3 = data[2][['position', 'velocity']].fillna(0)

y1 = data[0][['energy']].fillna(0)
y2 = data[1][['energy']].fillna(0)
y3 = data[2][['energy']].fillna(0)


y1[y1<0] = 0
y2[y2<0] = 0
y3[y3<0] = 0
```

 그 다음, 이 data에서 input과 output을 나누어 보았다. **이번 model의 목표는 position, velocity를 이용하여 energy를 예측하는 것**이므로 input이 position과 velocity를, output이 energy라고 볼 수 있다. 위와 같이 x, y를 정의하여 형식을 dataframe에서 numpy의 array로 바꿔 keras model이 사용할 수 있도록 하였다. 한편, 데이터에 포함된 N/A를 0으로 바꾸는 fillna(0)와 에너지가 0 이하인것은 불가능하므로 모두 0으로 변경하는 코드를 추가하여 데이터를 손질하였다.

```python
from sklearn.preprocessing import StandardScaler

x_scaler = StandardScaler()
y_scaler = StandardScaler()

x1 = x_scaler.fit_transform(x1).reshape(1,x1.shape[0],x1.shape[1])
x2 = x_scaler.transform(x2).reshape(1,x2.shape[0],x2.shape[1])
x3 = x_scaler.transform(x3).reshape(1,x3.shape[0],x3.shape[1])

y1 = y_scaler.fit_transform(y1).reshape(1,y1.shape[0],y1.shape[1])
y2 = y_scaler.transform(y2).reshape(1,y2.shape[0],y2.shape[1])
y3 = y_scaler.transform(y3).reshape(1,y3.shape[0],y3.shape[1])
```

 이후, 데이터 표준화 작업을 해줬다. StandardScaler는 어떤 인자도 넣지 않으면 평균 0, 분산이 1인 정규분포로 데이터를 재단해준다. 다만, reshape를 사용하는 이유가 있는데..

```python
x1 = x_scaler.fit_transform(x1)
print(x1.shape)
```

```
(2171,2) # (time step, feature)
```

 Scaler는 2차원만을 받아 표준화한다. 당연히 그 결과물도 2차원인데, 오늘 사용할 **LSTM은 3차원의 데이터를 받는다**는 점이 문제가 된다. 따라서,

```python
x1 = x_scaler.fit_transform(x1).reshape(1,x1.shape[0],x1.shape[1])
print(x1.shape)
```

```
(1, 2171, 2) # (size, time step, feature)
```

 이렇게 reshape를 반드시 해주어야 한다. 쉽게 이해하자면, 

- Size : 샘플(시간 스텝이 없는 데이터를 생각해보자)

- Time step : 샘플이 가지고 있는 시간 간격

- Feature : Input 또는 Output

과 같이 이해할 수 있다.

### LSTM 모델 정의 및 학습

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM
from tensorflow.keras import optimizers
from keras.callbacks import ReduceLROnPlateau , EarlyStopping
#이렇게 import하면 더 짧게 Sequential 등을 사용할 수 있다.

model = Sequential([LSTM(units = 50, return_sequences = True),
                    Dense(1)])
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='MSE', metrics=['MSE'])
reduceLR = ReduceLROnPlateau(monitor = 'val_MSE', patience = 10, factor = 0.9)
ES = EarlyStopping(monitor = 'val_MSE', mode='min', verbose=1, patience = 30, restore_best_weights=True)
```

 우선 LSTM의 중요 인자를 살펴보면,

- units : 차원 수, node 수와 비슷한 개념으로 통용되는 듯 하다.

- return_sequences : False가 default값으로, False로 설정하면 마지막 Time step의 결과만 출력하고 True로 설정한다면 Time step마다 결과물을 출력한다.

- initializer, activation : kernel_initializer, recurrent_initializer, activation, recurrent_activation 등 다양한 인자가 존재하는데, 각각 default값이 LSTM의 정의에 맞게 설정되어 있으므로 따로 입력하지 않아도 된다.

- dropout : LSTM은 인자로 dropout을 설정할 수 있는데, 이를 통해 정의하면 이전 시간을 반영하는 LSTM의 특성상 dropout 설정으로 생길 수 있는 문제를 어느정도 완화할 수 있다고 한다.

 이번 model의 목적에 부합하려면 time step에서 에너지가 어느정도인지 계산해내야 한다. 따라서 return_sequences를 True로 설정하였다.

 또, compile에서 loss를 MAE(Mean Absolute Error)로 설정하였다. 이는 MSE와 비교할 때 제곱하지 않는다는 점에서 outlier(전반적인 흐름에 어긋나는 데이터들을 의미한다. 보통 학습을 방해하지만 실제 내가 사용한 데이터에서는 어쩔 수 없이 포함시켜야 했다.)의 영향력이 줄어든다는 점에서 loss 함수로 채택하였다. 한편 Metrics에 MSE를 포함시켜 learning rate 감소 및 earlystopping을 설정하였다.

```python
model.fit(x1, y1, batch_size=32, epochs=100, callbacks = [reduceLR, ES], validation_data = (x2, y2))
```

 (x1, y1)을 train data로, (x2, y2)를 validation data로, (x3, y3)을 test data로 사용하여 학습을 완료했다.

### 결과 확인

```python
model.evaluate(x3,y3) #loss(MAE), MSE값을 볼 수 있다.

y_pred = model.predict(x3)
y_pred = y_pred.reshape(y_pred.shape[1],y_pred.shape[2])
y_pred = y_scaler.inverse_transform(y_pred)
y_test = y3
y_test = y_test.reshape(y_test.shape[1],y_test.shape[2])
y_test = y_scaler.inverse_transform(y_test)
```

 아까 scaler는 2차원의 data만 취급한다는 점을 강조하였는데, 이는 표준화를 풀 때에도 마찬가지이므로 데이터를 2차원으로 reshape해줘야 한다. 

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(32,24)) #graph의 사이즈

plt.subplot(311) #graph의 위치, 3x1 중 첫번째라는 뜻
plt.plot(y_pred)
plt.plot(y_test)
plt.xlabel('time')
plt.ylabel('W')
plt.title('energy')
plt.show
```

![data2](https://github.com/44isOutlaw/44isOutlaw.github.io/blob/master/assets/img/RNN/data3.png?raw=true)

 이렇게 해서 LSTM을 이용한 시계열 데이터를 분석 및 예측하는 model을 만들어 보았다. 다음 post에서는 이를 발전시킬 다양한 방법들에 대해 작성하도록 하겠다.
