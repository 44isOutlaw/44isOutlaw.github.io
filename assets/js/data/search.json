[ { "title": "Lstm을 이용한 데이터 분석", "url": "/posts/LSTM%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D/", "categories": "", "tags": "", "date": "2022-08-08 00:00:00 +0900", "snippet": "RNN을 이용한 시계열 데이터 분석Ⅰ. IntroMachine learning을 이용하여 시간축이 있는 데이터를 학습하기 위해서는 어느 데이터의 이전 시간, 그리고 다음 시간의 정보가 반영되어야만 한다. 하지만 기존의 NN은 한 샘플의 전후 데이터가 반영되지 않는 바, 이러한 시계열 데이터를 다루기 위한 새로운 model이 고안되었으니 이것이 Recurrent Neural Network(이하 RNN)이다. 이하 본문에서는 RNN 및 RNN에 속하는 유용한 model을 이용하여 시계열 데이터 분석 과정을 소개한다.Ⅱ. Recurrent Nerual Network(이하 내용은 “딥 러닝을 이용한 자연어 처리 입문” https://wikidocs.net/2288 을 기반으로 스스로 이해한 바를 작성하였습니다.)RNN의 특징을 우선 설명하면, 위 그림과 같이 셀(cell)이 있다는 점이다. 셀은 이전의 값을 기억하려고 하고, 이를 다음 셀로 전달한다. 즉 이전 시간의 정보를 다음 시간의 정보 학습에 반영시킬 수 있다는 것이다. 좌측 그림과 같이 현재의 셀은 이전 시간 셀로부터 h라는 값을 받으며, 이를 은닉 상태(hidden state)라고 표현한다. 전달받은 과거의 은닉 상태(ht-1)는 현재의 은닉 상태(ht)를 계산하는데 사용된다. 계산 과정은 NN와 같이 가중치(w) 및 bias(b)가 추가되어 activation function을 거치는 과정을 거친다. 이렇게 얻은 현재의 h는 출력값 y를 계산하는데 사용된다. 이와 같은 특징을 살려 시계열 데이터의 학습이 가능해지며, 데이터의 분석 뿐만 아니라 다음 시간의 출력값 예측(예를 들어, 주가 동향) 등 다양한 상황에 적용할 수 있다.하지만 위와 같은 방식의 RNN에는 단점이 있는데, 많은 시간 간격을 지날수록 초기의 은닉 상태값의 특징이 옅어진다는 점이다. 이는 곧 시간이 길어질수록 초반부의 데이터가 반영되지 않음을 의미하며 의도한 바와 다르게 model의 학습되는 결과를 야기한다. 이를 장기 의존성 문제(the problem of Long-Term Dependencies)라고 한다. 이를 해결하기 위해, RNN을 수정한 모델인 LSTM, GRU 등이 등장하게 된다.Ⅲ. LSTM, GRU시계열 데이터를 다루는 가장 대표적인 두 모델인 LSTM, GRU에 대하여 알아보자.위 그림은 Long Short-Term Memory(이하 LSTM)의 구조이다. LSTM은 기존 은닉 상태(h)와 새롭게 추가된 셀 상태(C)를 삭제 게이트, 입력 게이트, 출력 게이트를 통해 계산하여 불필요한 정보를 제거하고 기억해야 할 정보를 다음 상태로 전달시키는 과정을 수행한다. 현재의 셀 상태(Ct)는 삭제 게이트를 거쳐온 이전 셀 상태(Ct-1)와 입력 게이트를 거쳐온 이전 은닉 상태(ht-1)와 현재 입력값(x t )을 통해 계산되며, 현재의 은닉 상태(ht)는 이전 은닉 상태(ht-1)와 현재 입력값(xt)를 통해 계산되며, 최종 output에 도달할 때까지 반복된다. LSTM은 이와 같이 게이트들이 정보를 선별하면서 장기 의존성 문제를 완화하여, 초기 시간의 중요 정보들을 보존하여 학습을 수행할 수 있다.​ 한편, Gated Recurrent Unit(이하 GRU)의 구조는 위 그림과 같다. GRU는 LSTM과 달리 업데이트 게이트와 리셋 게이트를 갖고 있다. 두 게이트 모두 이전 은닉 상태(ht-1)와 현재 입력값(x t )이 지나가게 되는데, 업데이트 게이트를 지난 값(zt)이 이전 은닉 상태(ht-1)와 리셋 게이트와 현재 입력값(x t )을 거쳐 연산된 gt의 비중을 조절하여 현재 은닉 상태(ht)를 출력한다. 이와 같은 방식으로 이전의 중요 정보들을 보전하여 장기 의존성 문제를 완화한다. GRU는 LSTM과 비슷한 성능을 갖고 있으므로, LSTM의 최적화를 마쳤다면 굳이 GRU로 바꿀 필요가 없다고 한다.Ⅳ. Keras의 LSTM을 이용한 시계열 데이터 분석이번 model의 목표는 position, velocity를 입력 받으면 energy 소모량을 예측할 수 있도록 하는 것이다. 이하에는 그 과정을 기록한다.시계열 데이터의 처리 time position velocity energy 0 0 0 0 1 15 100 1000 2 67 360 3000 … … … … 오늘 사용하는 데이터는 공개하기에는 어려움이 있어, 예시로만 설명을 하고자 한다. 이 데이터는 시간에 따라 어떤 기계의 위치, 속도의 변화에 따른 에너지 소모량을 기록한 것이다. 이와 같이 일정 시간 간격마다 기록된 데이터를 시계열 데이터라고 한다. 이번엔 이렇게 생긴 엑셀 파일(.xlsx)을 구글 드라이브에서 import 받을 것이다.from google.colab import drivedrive.mount('/content/drive')위와 같은 코드를 통해 구글 드라이브에 접근할 수 있다. 이 코드를 작성하면 보안 알림이 뜨는데 모두 예를 누르면 된다. 이렇게 입력하고 colab의 좌측 파일 모양의 아이콘을 누르면,이와 같이 drive가 추가된 것을 볼 수 있다. 여기서는 내부 파일에 있는 MyDrive에 xlsx 파일을 업로드하여 import 하였으며, 사용된 코드는 아래와 같다.import pandas as pddata = [0]*3data[0] = pd.read_excel(\"/content/drive/MyDrive/data.xlsx\", engine = 'openpyxl', sheet_name='Sheet1')data[1] = pd.read_excel(\"/content/drive/MyDrive/data.xlsx\", engine = 'openpyxl', sheet_name='Sheet2')data[2] = pd.read_excel(\"/content/drive/MyDrive/data.xlsx\", engine = 'openpyxl', sheet_name='Sheet3')pd.read_excel을 통해 xlsx 파일을 pandas의 dataframe으로 받을 수 있다. 엑셀 파일이 여러 개의 sheet로 나뉘어 있다면 sheet_name을 인자로 하나씩 저장할 수 있다. 위 파일은 3개의 sheet가 있으며, data라는 이름의 list를 정의하여 하나씩 저장하였다.import numpy as npx1 = data[0][['position', 'velocity']].fillna(0)x2 = data[1][['position', 'velocity']].fillna(0)x3 = data[2][['position', 'velocity']].fillna(0)y1 = data[0][['energy']].fillna(0)y2 = data[1][['energy']].fillna(0)y3 = data[2][['energy']].fillna(0)y1[y1&lt;0] = 0y2[y2&lt;0] = 0y3[y3&lt;0] = 0그 다음, 이 data에서 input과 output을 나누어 보았다. 이번 model의 목표는 position, velocity를 이용하여 energy를 예측하는 것이므로 input이 position과 velocity를, output이 energy라고 볼 수 있다. 위와 같이 x, y를 정의하여 형식을 dataframe에서 numpy의 array로 바꿔 keras model이 사용할 수 있도록 하였다. 한편, 데이터에 포함된 N/A를 0으로 바꾸는 fillna(0)와 에너지가 0 이하인것은 불가능하므로 모두 0으로 변경하는 코드를 추가하여 데이터를 손질하였다.from sklearn.preprocessing import StandardScalerx_scaler = StandardScaler()y_scaler = StandardScaler()x1 = x_scaler.fit_transform(x1).reshape(1,x1.shape[0],x1.shape[1])x2 = x_scaler.transform(x2).reshape(1,x2.shape[0],x2.shape[1])x3 = x_scaler.transform(x3).reshape(1,x3.shape[0],x3.shape[1])y1 = y_scaler.fit_transform(y1).reshape(1,y1.shape[0],y1.shape[1])y2 = y_scaler.transform(y2).reshape(1,y2.shape[0],y2.shape[1])y3 = y_scaler.transform(y3).reshape(1,y3.shape[0],y3.shape[1])이후, 데이터 표준화 작업을 해줬다. StandardScaler는 어떤 인자도 넣지 않으면 평균 0, 분산이 1인 정규분포로 데이터를 재단해준다. 다만, reshape를 사용하는 이유가 있는데..x1 = x_scaler.fit_transform(x1)print(x1.shape)(2171,2) # (time step, feature)Scaler는 2차원만을 받아 표준화한다. 당연히 그 결과물도 2차원인데, 오늘 사용할 LSTM은 3차원의 데이터를 받는다는 점이 문제가 된다. 따라서,x1 = x_scaler.fit_transform(x1).reshape(1,x1.shape[0],x1.shape[1])print(x1.shape)(1, 2171, 2) # (size, time step, feature)이렇게 reshape를 반드시 해주어야 한다. 쉽게 이해하자면, Size : 샘플(시간 스텝이 없는 데이터를 생각해보자) Time step : 샘플이 가지고 있는 시간 간격 Feature : Input 또는 Output 과 같이 이해할 수 있다.LSTM 모델 정의 및 학습import tensorflow as tffrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Dropout, LSTMfrom tensorflow.keras import optimizersfrom keras.callbacks import ReduceLROnPlateau , EarlyStopping#이렇게 import하면 더 짧게 Sequential 등을 사용할 수 있다.model = Sequential([LSTM(units = 50, return_sequences = True), Dense(1)])model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='MSE', metrics=['MSE'])reduceLR = ReduceLROnPlateau(monitor = 'val_MSE', patience = 10, factor = 0.9)ES = EarlyStopping(monitor = 'val_MSE', mode='min', verbose=1, patience = 30, restore_best_weights=True)우선 LSTM의 중요 인자를 살펴보면, units : 차원 수, node 수와 비슷한 개념으로 통용되는 듯 하다. return_sequences : False가 default값으로, False로 설정하면 마지막 Time step의 결과만 출력하고 True로 설정한다면 Time step마다 결과물을 출력한다. initializer, activation : kernel_initializer, recurrent_initializer, activation, recurrent_activation 등 다양한 인자가 존재하는데, 각각 default값이 LSTM의 정의에 맞게 설정되어 있으므로 따로 입력하지 않아도 된다. dropout : LSTM은 인자로 dropout을 설정할 수 있는데, 이를 통해 정의하면 이전 시간을 반영하는 LSTM의 특성상 dropout 설정으로 생길 수 있는 문제를 어느정도 완화할 수 있다고 한다. 이번 model의 목적에 부합하려면 time step에서 에너지가 어느정도인지 계산해내야 한다. 따라서 return_sequences를 True로 설정하였다.또, compile에서 loss를 MAE(Mean Absolute Error)로 설정하였다. 이는 MSE와 비교할 때 제곱하지 않는다는 점에서 outlier(전반적인 흐름에 어긋나는 데이터들을 의미한다. 보통 학습을 방해하지만 실제 내가 사용한 데이터에서는 어쩔 수 없이 포함시켜야 했다.)의 영향력이 줄어든다는 점에서 loss 함수로 채택하였다. 한편 Metrics에 MSE를 포함시켜 learning rate 감소 및 earlystopping을 설정하였다.model.fit(x1, y1, batch_size=32, epochs=100, callbacks = [reduceLR, ES], validation_data = (x2, y2))(x1, y1)을 train data로, (x2, y2)를 validation data로, (x3, y3)을 test data로 사용하여 학습을 완료했다.결과 확인model.evaluate(x3,y3) #loss(MAE), MSE값을 볼 수 있다.y_pred = model.predict(x3)y_pred = y_pred.reshape(y_pred.shape[1],y_pred.shape[2])y_pred = y_scaler.inverse_transform(y_pred)y_test = y3y_test = y_test.reshape(y_test.shape[1],y_test.shape[2])y_test = y_scaler.inverse_transform(y_test)아까 scaler는 2차원의 data만 취급한다는 점을 강조하였는데, 이는 표준화를 풀 때에도 마찬가지이므로 데이터를 2차원으로 reshape해줘야 한다.import matplotlib.pyplot as pltplt.figure(figsize=(32,24)) #graph의 사이즈plt.subplot(311) #graph의 위치, 3x1 중 첫번째라는 뜻plt.plot(y_pred)plt.plot(y_test)plt.xlabel('time')plt.ylabel('W')plt.title('energy')plt.show이렇게 해서 LSTM을 이용한 시계열 데이터를 분석 및 예측하는 model을 만들어 보았다. 다음 post에서는 이를 발전시킬 다양한 방법들에 대해 작성하도록 하겠다." }, { "title": "Dnn을 이용한 데이터 분류", "url": "/posts/DNN%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EB%A5%98/", "categories": "", "tags": "", "date": "2022-07-25 00:00:00 +0900", "snippet": "DNN을 이용한 데이터 분류Ⅰ. IntroMachine learning 기법 중 하나인 NN(Neural Network)에 대하여 간단히 알아보고, 이를 구현한 keras 모델을 이용해 데이터를 분류하는 과정을 본문에 정리했다.Ⅱ. Neural NetworkNeural Network(이하 NN)은 Machine learning 기법 중 하나로, 인간의 뇌세포를 모방하여 만들어졌다. 여러가지 입력 신호를 받아 하나의 결과를 출력하는 아이디어를 채용하여, 아래 사진과 같은 모양을 만들어냈다.(출처 : www.ibm.com)NN은 input layer, hidden layer 그리고 output layer로 구성되어 있으며, 각 layer의 node는 양 옆의 layer의 모든 node들과 연결되어 있다. NN의 학습 과정을 간단히 설명하면, input layer가 데이터를 받아(입력 신호에 대응됨) hidden layer에서 일련의 연산을 거쳐, output layer로 결과를 출력한다. 여기서 hidden layer의 개수에 따라, 구조에 따라 명칭이 달라지는데, 위 그림과 같이 hidden layer가 여러개인 경우에는 Deep Neural Network(이하 DNN)이라고 한다.그렇다면 이러한 NN의 hidden layer에서 무슨 일이 일어나는지 궁금해지는데, 각 node는 위 그림과 같이 input data(위 그림의 x)를 받아 가중치(위 그림의 w)를 곱하고 bias(b)를 더한 후 activation function을 거쳐 나온 Y를 다음 node에 전달한다. 이 때, activation function은 relu, sigmoid, tanh, softmax 등등 다양한 종류가 있으며 가중치 및 bias와 연산을 거친 값을 특정한 값으로 출력한다. 다루는 dataset에 따라 적절한 activation function을 골라야 하는데, 기회가 된다면 후술하도록 한다. (https://keras.io/ko/activations/ 에 keras에서 이용 가능한 activation function이 정리되어 있음)다시 돌아와서, NN을 학습한다는 것은 cost function을 최소화하는 가중치 w를 구하는 것을 의미한다. Cost function은 우리가 익히 알고있는 Mean Squared Error(이하 MSE)등의 함수로, 우리가 학습 과정에서 model이 지향하도록 제시하는 output값과 model이 예측하는 output값을 비교하는 함수이다.(이렇게 우리가 결과값을 제시하여 학습시키기 때문에 이를 supervised learning이라고 표현한다)위 그림과 같이 cost function J에서 w값을 조정하여 J가 최소가 되는 w를 찾아내는 과정은 J의 w에 대한 편미분값의 일정 비율을 빼는 방식을 따르며, 이 비율을 learning rate라고 한다. Learning rate을 설정할 때는 너무 작으면 학습 시간이 오래 걸리고, 너무 크면 최소가 되는 지점을 벗어날 수 있다는 점을 유의하여야 한다. 이러한 과정을 output layer에서부터 역으로 layer들을 지나며 w값을 업데이트하는 방식으로 학습이 진행된다.(이를 backpropagation이라고 한다)Ⅲ. Keras의 DNN ModelKeras는 tensorflow가 제공하는 python 기반 machine learning model이다. 이하는 Keras를 scikit-learn의 분류 dataset에 직접 적용한 과정을 소개하면서 어떤 과정을 거쳐 model을 만들고 학습시키는지 작성하도록 한다.숫자 손글씨 분류 Modelimport numpy as np import pandas as pd #np,pd 모두 data를 손질하기 위한 toolimport matplotlib.pyplot as plt #이미지 등을 plot를 위한 modulefrom sklearn.datasets import load_digits #이번 예제의 datasetfrom sklearn import preprocessing #data 정규화를 위한 modulefrom sklearn.model_selection import train_test_split #train용,test용으로 데이터를 나눔import tensorflow as tf #tensorflow 안에 keras가 있다import random사실 숫자 손글씨를 분류하는 dataset는 MNIST라는 좋은 dataset이 있지만, MNIST는 data가 3차원으로 되어 있어 DNN으로는 다루기 힘드므로, 이번엔 scikit-learn에 있는 2차원 dataset을 이용했다. (MNIST는 추후 업로드할 CNN의 예제에서 소개하도록 한다.)Data 손질digits = load_digits()x = digits.datay = digits.target위 코드를 통해 dataset를 불러올 수 있다. load_digits()로 불러낸 dataset는 data와 target으로 구성되어 있으며 각각 x, y으로 정의하였다. 이를 뜯어보면..print(digits.data.shape)print(digits.target.shape) #shape는 np의 함수로, 배열의 구조를 보여줌print(digits.data)print(digits.target)plt.gray() #회색으로 이미지 출력for i in range(1,15): plt.matshow(digits.images[i]) #dataset의 1번부터 14번까지 이미지 출력(1797, 64) #(샘플 수, input 수)(1797,) #(샘플 수, output 수)[[ 0. 0. 5. ... 0. 0. 0.] [ 0. 0. 0. ... 10. 0. 0.] [ 0. 0. 0. ... 16. 9. 0.] ... [ 0. 0. 1. ... 6. 0. 0.] [ 0. 0. 2. ... 12. 0. 0.] [ 0. 0. 10. ... 12. 1. 0.]] #(data의 내용)[0 1 2 ... 8 9 8] #(target의 내용)&lt;Figure size 432x288 with 0 Axes&gt; 이 결과로 미루어 보아 dataset에 1797의 sample이 있으며, input 수는 64개이며 input는 0~16 사이의 정수로 이루어져 있음을 짐작할 수 있다. 이 64개의 input는 숫자 사진을 8x8로 나누어 각 구역의 명도를 나타내고 있다.(숫자가 높을수록 밝은 듯 하다.) 한편, target의 output은 내용으로 보아 그림이 의도한 수가 어떤 숫자인지 나타내고 있는 듯 한데, 구조가 (1797,)으로 뭔가 이상함을 느낄 수 있다. 다만 target의 output이 1개임은 자명하므로, reshape 함수를 이용하여 해결할 수 있다.y = digits.target.reshape(1797,1) #구조를 (1797,1)로 바꾸기print(y.shape)(1797, 1) #적절히 수정되었음을 알 수 있다.이제 우리는 dataset이 어떤 모양인지 파악했다. 하지만, input은 0~16 사이 정수이며 output은 0~9 사이 정수이므로 어느정도 차이가 있는 상태인데, 이 수치 그대로 model에 학습시킨다면 가중치(w)가 의도와 다르게 학습될 수 있다. 이를 해결하기 위해 우리는 데이터를 정규화해주어야 한다. (솔직히 이러한 dataset에도 정규화가 필요한지에는 의문점이 남아있으나, 전혀 성질, 범위가 다른 input이 결합되어 있는 dataset이라면 이 과정을 필수적으로 수행해야 적절히 model을 학습시킬 수 있으므로 유의해야 한다)min_max_scaler = preprocessing.MinMaxScaler() #preprocessing module 속 MinMaxScaler 메소드x = min_max_scaler.fit_transform(x) #fit-수치를 조정 transform-적용. fit_transform은 동시에y = min_max_scaler.fit_transform(y)MinMaxScaler()는 데이터들을 0~1사이 수치로 조정해준다. 이 메소드는 인자로 feature_range=(a,b)를 받아 a,b사이로 수치를 조정할 수 있다. 본문에서는 fit_transform을 이용하여 x,y를 0~1사이로 정규화 시켰다. scikit-learn의 preprocessing module은 MinMaxScaler 뿐만 아니라, StandardScaler, RobustScaler 등 다양한 정규화 함수를 갖고 있으니 상황에 맞게 사용하기를 권장한다.마지막으로, 이렇게 손질한 data를 train_test_split 메소드로 train용, test용으로 나누어야 한다. 이렇게 train용, test용으로 데이터를 나누는 이유는 학습의 결과가 잘 나타났는지 확인하기 위함이다.(모델을 학습시킨 데이터로 test해봤자 당연히 좋은 결과가 나오기 때문이다)img = digits.images #나중에 test 이미지를 plot하기 위해 미리 정의했다seed = 42np.random.seed(seed) #난수가 매번 같은 수를 가짐x_train, x_test, y_train, y_test, img_train, img_test = train_test_split(x, y, img, random_state = seed, test_size = 0.1) #x,y,img를 train과 test로 나눔이와 같이 train_test_split를 정의하면 x, y, img에서 test_size의 비율만큼 test용 data, train용 data로 분류한다. 이 때, random_state가 전체 dataset에서 무작위로 test data를 수집하도록 하며, 이에 앞서 np.random.seed를 정의하여 x, y, img에서 같은 위치의 data를 수집하도록 해야한다. 이를 정의하지 않는 경우 random의 숫자가 x를 구할때와 y, img를 구할 때와는 달라져 x, y, img의 test값이 서로 일치하지 않는 결과가 발생하게 된다. 이렇게 data를 나누는 것에 성공했다면, dataset의 손질은 끝났고 모든 준비를 마친 것이다.Model 정의 및 학습Keras model은 두가지 방법으로 정의할 수 있다. 이하의 2개의 모델 정의 방법은 하나의 예시로 참고하면 된다._initializer = 'glorot_normal'_activation = 'relu'model = tf.keras.Sequential([ tf.keras.layers.Dense(128, kernel_initializer = _initializer, activation = _activation, input_shape = (64,)), tf.keras.layers.Dense(512, kernel_initializer = _initializer, activation = _activation), tf.keras.layers.Dense(1)])_initializer = 'glorot_normal'_activation = 'relu'input = tf.keras.layers.Input(shape=(64,))D = tf.keras.layers.Dense(128, kernel_initializer = _initializer, activation= _activation)(input)D = tf.keras.layers.Dense(512, kernel_initializer = _initializer, activation= _activation)(D)output = tf.keras.layers.Dense(1)(D)model = tf.keras.models.Model(inputs=input, outputs=output)아래의 메소드를 이용하여 model의 구조를 한 눈에 확인할 수 있다.model.summary()tf.keras.utils.plot_model(model, show_shapes=True)Model: \"sequential_2\"_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_31 (Dense) (None, 128) 8320 dense_32 (Dense) (None, 512) 66048 dense_33 (Dense) (None, 1) 513 =================================================================Total params: 74,881Trainable params: 74,881Non-trainable params: 0_________________________________________________________________Model: \"model_3\"_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_9 (InputLayer) [(None, 64)] 0 dense_25 (Dense) (None, 128) 8320 dense_26 (Dense) (None, 512) 66048 dense_27 (Dense) (None, 1) 513 =================================================================Total params: 74,881Trainable params: 74,881Non-trainable params: 0_________________________________________________________________보다시피, 두 model의 정의 방법은 전혀 다르지만 실제로 model의 형태는 동일하다. Initializer, activation은 일단 무시하고(실제로 없어도 정의가 되긴 한다) 구조를 정의하는 방법의 차이에 대해 설명하면, Sequential은 그 내부에 정의할 layer가 한 줄로 이어지도록 사전에 의도하는 것이다. 한편 input부터 차례대로 정의하는 방법은 의도하면 여러 input, output을 설정할 수도 있고 여러 개의 NN model을 병렬 구조로 합칠수도 있어 높은 자유도를 갖지만, sequential에 비해 정의하기에 상대적으로 번거롭다.이렇게 생긴 model은 후자의 방법으로 정의가 가능하다. 다만 이번에는 한 줄의 DNN 구조를 정의하고자 하므로, Sequential 함수를 이용하여 model을 정의하도록 한다.Sequential 내부를 살펴보면, Dense layer가 있는데 이는 hidden layer 또는 output layer로 정의될 수 있다. Dense의 맨 처음 인자를 통해 node 수를 정의할 수 있으며, 첫번째 dense layer에서는 input_shape 인자로 input의 모양을 설정할 수 있다.(설정하지 않아도 입력되는 input을 알아서 감지하긴 한다.) 특히 output 역할을 하는 dense layer는 output 개수와 동일한 node를 갖도록 정의하여야 한다. 이외에는 자유롭게 Dense를 더 늘려 layer의 개수를 늘리거나, node 수를 변경해가며 dataset에 최적화된 model을 찾는 것 뿐이다.tf.keras.layers.Dense(128, kernel_initializer = _initializer, activation= _activation)이제 initializer, activation에 대해 설명한다. Activation은 처음에 짐작했듯이, activation function의 설정이다. 설정하지 않아도 dense layer 정의는 가능하나, activation을 설정하지 않으면 정상적인 학습은 불가능하다. 본문에서는 ‘relu’라는 activation function을 설정하였는데, 이는 REctified Linear Unit의 줄임으로 0 이하의 값은 모두 0으로, 0 이상은 선형이 되도록 수치를 변경하는 activation function이다. 우리의 dataset의 output이 0~9(물론 정규화되었지만)임을 고려하면, output이 왜곡될 가능성이 낮아보인다.Initializer는 가중치(w)들의 초기값을 설정하는 인자이다. 예측할 수는 없겠지만, 초기값이 최종 가중치값과 너무 다르다면 학습에 어려움이 따를 수 있음을 예상할 수 있다. 반대로 말하면, 초기값이 잘 설정된다면 model의 학습에 크게 기여할 수 있다는 것이다. 본문에서 설정한 ‘glorot_normal’은 Xavier normal initializer라고도 불리는 가장 보편적인 initializer로, 이전 hidden layer의 노드 수와 현재의 노드 수를 이용해 계산한 값을 표준편차로 가중치를 정규화시킨다. 이렇게 initializer, activation을 hidden layer에 적용시켜 model을 정의하였다.drop_rate = 0.1_initializer = 'glorot_normal'_activation = 'relu'model = tf.keras.Sequential([ tf.keras.layers.Dense(128, kernel_initializer = _initializer, activation = _activation, input_shape = (64,)), tf.keras.layers.Dense(512, kernel_initializer = _initializer, activation = _activation), tf.keras.layers.Dropout(drop_rate), tf.keras.layers.Dense(64, kernel_initializer = _initializer, activation = _activation), tf.keras.layers.Dense(1)])Model의 정의를 마치기 전에, 성능 향상을 위한 장치를 하나 소개한다. 바로 Dropout layer인데, 이는 무작위로 가중치(w)의 값을 입력된 비율만큼 제거한다. 즉, Dropout layer는 model이 train data을 과하게 학습하여 test data의 target를 맞추지 못하는(overfitting) 상황을 가중치를 고의적으로 버려 완화시켜 줌으로써 model의 성능을 향상시킬 수 있다. 만약 model이 overfitting되고 있다면, dropout layer를 사용하는 것을 고려해야 할 것이다._optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)model.compile(optimizer = _optimizer, loss = 'MSE', metrics=['accuracy'])Model의 정의가 완료되면 학습시키기 전에 compile 작업이 필요하다. Compile에서는 최적의 w를 찾을 방법(optimizer)를 설정해주어야 하며, cost function(loss)을 정의한다. metrics에는 성능지표로 이용할 함수를 정의해줄 수 있는데, 본문과 같이 분류학습의 경우에는 실제 target값과 학습 target값을 비교하는 ‘accuracy’가 적절한 지표가 될 수 있다. 여기서 optimizer는 Adam으로 정의하였는데 이는 Ⅱ절의 gradient 방식의 일종으로 가장 보편적으로 사용되는 optimizer이다. Optimizer는 learning_rate를 설정해주어야 하는데, 이 값이 너무 크면 loss function의 최저값을 지나칠수도 있으며 너무 작으면 학습 시간이 매우 오래 걸릴 수 있으므로 적절한 값을 설정하여야 한다. loss는 익숙한 MSE로 설정하였다._batch_size = 64reduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', patience = 10, factor = 0.9)ES = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 200, restore_best_weights = True)history = model.fit(x_train, y_train, epochs = 3000, batch_size = _batch_size, callbacks = [reduceLR, ES], validation_split = 0.1, verbose = 1)드디어 model을 학습시킬 수 있게 되었다. Model의 학습은 fit 함수를 이용하여 수행하며 첫번째와 두번째 인자로 x_train과 y_train을 입력해야 한다. epochs는 전체 데이터셋을 1회 학습한 상태로, 학습 횟수를 의미한다. batch size는 한번에 다루는 data의 갯수를 의미한다. x, y값 및 epochs, batch size는 필수적으로 정의되어야 한다. Callbacks와 validation_split 등의 인자는 model의 학습을 돕는 장치들로 선택적으로 정의하면 된다.먼저, model.fit을 history에 정의하는 이유는 epoch마다 결과값을 확인할 수 있도록 출력하기 위함이다. 맨 뒤의 인자인 verbose가 1일 때 결과값이 출력되는데, verbose를 0으로 설정하면 학습이 끝날때까지 어떤 메세지도 출력하지 않는다. 다음으로, Epochs는 model이 overfitting되거나 underfitting되지 않도록 적절히 설정해야 하며, batch size는 너무 작으면 학습 속도가 느려지고, 너무 크면 메모리의 한계로 문제가 발생할 수 있다는 점을 유의하여야 한다.Fit의 인자중에는 validation_split이 있는데, 이를 설정하면 train data의 0.1(입력값)의 비율만큼은 validation을 수행하는데 사용하게 된다. Validation 과정은 epoch마다 학습된 model에 validation data를 입력하여 평가하게 되며, 위 compile에서 설정한 그대로 validation으로 계산한 loss(val_loss)와 accuracy(val_accuracy)를 추가적으로 출력하게 된다. 이러한 출력물들은 학습 과정에서 나오는 loss와 accuracy보다 신뢰성이 있으며 model이 overfitting되지 않고 적절히 학습하고 있는지 평가할 수 있는 중요한 지표가 된다.Callbacks에는 [reduceLR, ES]를 정의하였는데, reduceLR은 ReduceLROnPlateau라는 메소드를 정의한 것으로, monitor(‘val_loss’)가 patience(10) epochs 동안 성능이 좋아지지 않는다면 learning rate(compile에서 정의되었다)를 factor(0.9)배 하겠다는 의미이다. 그리고 ES는 EarlyStopping라는 메소드를 정의한 것으로, monitor(‘val_loss’)가 patience(200) epochs동안 mode(여기서는 ‘min’으로, 최소값이 아니라면 작동한다는 뜻이다.)를 기준으로 삼아 만족하지 못한다면 학습을 일찍 종료하는 메소드이다. 또한 restore_best_weight를 True로 설정하면 이전 기록중에서 val_loss가 mode에 가장 부합하는 epoch의 가중치들을 최종 학습결과로 가져오게 된다. 위 두 메소드는 overfitting을 해소하기에 매우 적합한 방법들로, 본인의 model이 overfitting되고 있다면 사용함이 바람직하다.이렇게 설정한 뒤 실행하면 다음과 같은 메세지들이 출력된다.Epoch 1/300023/23 [==============================] - 3s 10ms/step - loss: 1.1988 - accuracy: 0.0976 - val_loss: 0.1234 - val_accuracy: 0.1235 - lr: 0.0100Epoch 2/300023/23 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.1636 - val_loss: 0.0848 - val_accuracy: 0.2037 - lr: 0.0100Epoch 3/300023/23 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.1883 - val_loss: 0.0496 - val_accuracy: 0.1975 - lr: 0.0100Epoch 4/300023/23 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.1876 - val_loss: 0.0423 - val_accuracy: 0.1975 - lr: 0.0100...(중략)Epoch 400/300023/23 [==============================] - 0s 4ms/step - loss: 3.0501e-04 - accuracy: 0.1945 - val_loss: 0.0185 - val_accuracy: 0.1975 - lr: 3.4337e-04Epoch 401/300019/23 [=======================&gt;......] - ETA: 0s - loss: 3.2039e-04 - accuracy: 0.1990Restoring model weights from the end of the best epoch: 201.23/23 [==============================] - 0s 4ms/step - loss: 3.9314e-04 - accuracy: 0.1945 - val_loss: 0.0193 - val_accuracy: 0.1975 - lr: 3.4337e-04Epoch 401: early stoppingLearning rate(lr)가 epoch가 진행될수록 낮아지고, 401 epoch에서 학습이 멈춘 것으로 보아 early stopping 모두 잘 작동함을 알 수 있다. Loss가 학습이 진행될 수록 낮아지고 있고, accuracy 또한 미약하지만 증가하고 있으므로 학습이 잘 진행었다. 여기서 accuracy가 왜이렇게 낮은지 의문점을 가질 수 있는데, 변명같지만 dataset을 보면 사람도 구분이 어려운 숫자가 여럿 있기 때문에 그런 것이라고 해두자.학습 결과 확인import random #test data을 랜덤으로 뽑기 위해 importy_pred = model.predict(x_test) #학습된 model에 test x값을 넣어 예측값을 구함real_ytest = min_max_scaler.inverse_transform(y_test).astype('int64')real_ypred = min_max_scaler.inverse_transform(y_pred).round().astype('int64')sample = range(y_pred.shape[0])sample = random.sample(sample, 10)for i in sample: plt.matshow(img_test[i]) plt.tick_params(labelsize=0) plt.title(\"actual num :%d\" %real_ytest[i]) plt.xlabel(\"predict num : %d\" %real_ypred[i])plt.show()Data 손질 과정에서 x,y값을 min_max scaler로 정규화 해준 것을 기억할 것이다. 이렇게 정의한 정규화는 inverse_transform을 통해 원상태로 돌릴 수 있다. 뒤에 astype(‘int64’)(타입을 정수로 바꾸기), round()(반올림 메소드)는 dataset의 output이 정수임을 고려하여 추가한 것이다. 이하 sample에서는 예측값을 랜덤하게 10개 선택하도록 코드를 작성하여 학습이 잘 이루어졌는지 확인하는 과정을 가졌다. 위 코드를 입력하면 이러한 결과물이 나오는데, 분명 몇개는 잘 맞추지만 틀리는 것도 존재한다. 하지만 왜 틀렸는지 납득이 간다. Accuracy가 이상하게 값이 낮은 이유가 있다고 생각된다.score = model.evaluate(x_test, y_test)6/6 [==============================] - 0s 3ms/step - loss: 0.0041 - accuracy: 0.2278또한, 정량적인 지표는 evaluate를 이용하여 얻을 수 있다. Test값을 이용하여 얻은 loss 또는 accuracy값은 model의 학습 정도의 정량적인 지표로 이용할 수 있을 것이다.Ⅳ. 결론이렇게 본문에서는 NN 및 DNN에 대하여, 그리고 keras를 이용하여 DNN model을 설계하는 과정을 살펴보았다. 다른 Machine learning model을 만드는 경우에도Data 손질Model 정의 및 학습학습 결과 확인과 같은 process를 거의 벗어나지 않으므로, 이를 사용하는데 크게 어려움이 없다. 하지만, model의 정확도를 향상시키는 방법은 조금 어려움이 있으며, 타인이 비슷한 상황에서 만든 model을 참고하고 keras의 내부 기능을 최대한 활용하여 오랜 시간 진행해보아야 할 것이라고 생각된다." } ]
